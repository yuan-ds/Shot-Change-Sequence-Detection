{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7badc3cb",
      "metadata": {
        "id": "7badc3cb"
      },
      "source": [
        "## 1. Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "AuO32Xm2fb1z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuO32Xm2fb1z",
        "outputId": "d67ef849-65d1-4a02-bf1d-80e005d5a380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "jIlv-QiNgNt7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIlv-QiNgNt7",
        "outputId": "f3c682c7-9c68-4bec-fa0a-f2b0845adf28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!cp /content/gdrive/MyDrive/converted_224x224.tar.gz /content\n",
        "%cd /content\n",
        "!tar -xzf converted_224x224.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "812ec14e",
      "metadata": {
        "id": "812ec14e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pdb\n",
        "import gc\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision \n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "# from torchinfo import summary\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c5972057",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5972057",
        "outputId": "2d011489-a1bc-4b9c-cbe0-b550107343fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda =  False  with num_workers =  0  system version =  3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ],
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "num_workers = 8 if cuda else 0\n",
        "\n",
        "print(\"Cuda = \", str(cuda), \" with num_workers = \", str(num_workers),  \" system version = \", sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71e53e8",
      "metadata": {
        "id": "e71e53e8"
      },
      "source": [
        "## 2. Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bdee1e8",
      "metadata": {
        "id": "3bdee1e8"
      },
      "source": [
        "### 2.1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6e874ee1",
      "metadata": {
        "id": "6e874ee1"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "df = pd.read_csv(\"4+1.csv\")\n",
        "df = df.iloc[:, 1:]\n",
        "\n",
        "train_end = int(len(df)*0.7)\n",
        "val_end = int(len(df)*0.85)\n",
        "train_data = df[:train_end]\n",
        "val_data = df[train_end:val_end]\n",
        "test_data = df[val_end:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e669450b",
      "metadata": {
        "id": "e669450b"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.reset_index().drop('Unnamed: 0.1',1).drop('index',1)\n",
        "val_data = val_data.reset_index().drop('Unnamed: 0.1',1).drop('index',1)\n",
        "test_data = test_data.reset_index().drop('Unnamed: 0.1',1).drop('index',1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "rGYT1RmDl-fG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "rGYT1RmDl-fG",
        "outputId": "ea39d6bf-5c7c-46fa-b225-b35fea94b0a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>frames</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>('frame_149670.jpg', 'frame_149739.jpg', 'fram...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>('frame_149738.jpg', 'frame_149692.jpg', 'fram...</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>('frame_149729.jpg', 'frame_149716.jpg', 'fram...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>('frame_149669.jpg', 'frame_149712.jpg', 'fram...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>('frame_149683.jpg', 'frame_149707.jpg', 'fram...</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17942</th>\n",
              "      <td>('frame_180809.jpg', 'frame_174579.jpg', 'fram...</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17943</th>\n",
              "      <td>('frame_174581.jpg', 'frame_174634.jpg', 'fram...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17944</th>\n",
              "      <td>('frame_174605.jpg', 'frame_174596.jpg', 'fram...</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17945</th>\n",
              "      <td>('frame_181123.jpg', 'frame_174585.jpg', 'fram...</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17946</th>\n",
              "      <td>('frame_181333.jpg', 'frame_174579.jpg', 'fram...</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17947 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  frames  label\n",
              "0      ('frame_149670.jpg', 'frame_149739.jpg', 'fram...     16\n",
              "1      ('frame_149738.jpg', 'frame_149692.jpg', 'fram...     80\n",
              "2      ('frame_149729.jpg', 'frame_149716.jpg', 'fram...     86\n",
              "3      ('frame_149669.jpg', 'frame_149712.jpg', 'fram...      7\n",
              "4      ('frame_149683.jpg', 'frame_149707.jpg', 'fram...     34\n",
              "...                                                  ...    ...\n",
              "17942  ('frame_180809.jpg', 'frame_174579.jpg', 'fram...    101\n",
              "17943  ('frame_174581.jpg', 'frame_174634.jpg', 'fram...     14\n",
              "17944  ('frame_174605.jpg', 'frame_174596.jpg', 'fram...     54\n",
              "17945  ('frame_181123.jpg', 'frame_174585.jpg', 'fram...     96\n",
              "17946  ('frame_181333.jpg', 'frame_174579.jpg', 'fram...     97\n",
              "\n",
              "[17947 rows x 2 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15cbf79",
      "metadata": {
        "id": "f15cbf79"
      },
      "source": [
        "### 2.2 Custom Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "RK6gHTVopf7M",
      "metadata": {
        "id": "RK6gHTVopf7M"
      },
      "outputs": [],
      "source": [
        "# cur_dir = \"data/qscale31_unique/\"\n",
        "cur_dir = \"/content/converted_224x224/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c21554c7",
      "metadata": {
        "id": "c21554c7"
      },
      "outputs": [],
      "source": [
        "# Define dataset class\n",
        "class MyDataSet(Dataset):\n",
        "\n",
        "    # load the dataset\n",
        "    def __init__(self, data, transform=None, **kwargs):\n",
        "        self.X = data[\"frames\"]\n",
        "        self.Y = data[\"label\"]\n",
        "        self.transform = transform\n",
        "\n",
        "    # get number of items/rows in dataset\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.X[index], self.Y[index]\n",
        "        for c in [\"(\",\")\",\",\",\"'\"]:\n",
        "            x = x.replace(c, \"\")\n",
        "        x = x.split(\" \")\n",
        "        images = []\n",
        "        for img_file in x:\n",
        "            img = Image.open(cur_dir + img_file)\n",
        "            img = torchvision.transforms.ToTensor()(img)\n",
        "            # img = torchvision.transforms.Normalize(mean=[0.2068, 0.2242, 0.2269], std=[0.2520, 0.2394, 0.2320])(img)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            images.extend(img)\n",
        "        x = torch.stack(images)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "mQAByEibn5iz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQAByEibn5iz",
        "outputId": "39968179-6bb7-4c7c-b9f6-d1921cb605ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([15, 224, 224])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set = MyDataSet(train_data)\n",
        "train_set[0][0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6b5efe",
      "metadata": {
        "id": "6d6b5efe"
      },
      "source": [
        "### 2.3 Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "T97n1WeOtugc",
      "metadata": {
        "id": "T97n1WeOtugc"
      },
      "outputs": [],
      "source": [
        "# transform_train = torchvision.transforms.Compose([\n",
        "#     torchvision.transforms.Normalize(mean=[0.4967, 0.4858, 0.4584, 0.4969, 0.4700, 0.4585, 0.4964, 0.4854, 0.4580, 0.4972, 0.4862, 0.4585, 0.4964, 0.4857, 0.4584],\n",
        "#                                       std=[0.2681, 0.2574, 0.2433, 0.2678, 0.2500, 0.2429, 0.2679, 0.2572, 0.2428, 0.2680, 0.2573, 0.2428, 0.2680, 0.2574, 0.2431])\n",
        "# ])\n",
        "\n",
        "transform_train = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.15)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4fb2a207",
      "metadata": {
        "id": "4fb2a207"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# training data\n",
        "train_set = MyDataSet(train_data, transform_train)\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=8)\n",
        "\n",
        "# validation data\n",
        "val_set = MyDataSet(val_data)\n",
        "val_loader = DataLoader(val_set, shuffle=False, batch_size=batch_size, num_workers=8)\n",
        "\n",
        "# test data\n",
        "test_set = MyDataSet(test_data)\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "s_D-uhZUpRzd",
      "metadata": {
        "id": "s_D-uhZUpRzd"
      },
      "outputs": [],
      "source": [
        "# def batch_mean_and_sd(loader, num_channels):\n",
        "    \n",
        "#     cnt = 0\n",
        "#     fst_moment = torch.empty(num_channels)\n",
        "#     snd_moment = torch.empty(num_channels)\n",
        "\n",
        "#     for images, _ in tqdm(loader):\n",
        "#         b, c, h, w = images.shape\n",
        "#         nb_pixels = b * h * w\n",
        "#         sum_ = torch.sum(images, dim=[0, 2, 3])\n",
        "#         sum_of_square = torch.sum(images ** 2,\n",
        "#                                   dim=[0, 2, 3])\n",
        "#         fst_moment = (cnt * fst_moment + sum_) / (cnt + nb_pixels)\n",
        "#         snd_moment = (cnt * snd_moment + sum_of_square) / (cnt + nb_pixels)\n",
        "#         cnt += nb_pixels\n",
        "\n",
        "#     mean, std = fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)        \n",
        "#     return mean,std\n",
        "  \n",
        "# num_channels = 15\n",
        "# mean, std = batch_mean_and_sd(train_loader, num_channels)\n",
        "# print(\"mean and std: \\n\", mean, std)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a589effb",
      "metadata": {
        "id": "a589effb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "UUgfy9UUkavf",
      "metadata": {
        "id": "UUgfy9UUkavf"
      },
      "outputs": [],
      "source": [
        "# Code based on https://pytorch.org/vision/stable/_modules/torchvision/models/regnet.html\n",
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from typing import Any, Callable, List, Optional, Tuple\n",
        "from torch import nn, Tensor\n",
        "\n",
        "__all__ = [\"RegNet\", \"regnet_y_400mf\", \"regnet_y_800mf\", \"regnet_y_1_6gf\",\n",
        "           \"regnet_y_3_2gf\", \"regnet_y_8gf\", \"regnet_y_16gf\", \"regnet_y_32gf\",\n",
        "           \"regnet_x_400mf\", \"regnet_x_800mf\", \"regnet_x_1_6gf\", \"regnet_x_3_2gf\",\n",
        "           \"regnet_x_8gf\", \"regnet_x_16gf\", \"regnet_x_32gf\"]\n",
        "\n",
        "class ConvNormActivation(torch.nn.Sequential):\n",
        "    \"\"\"\n",
        "    Configurable block used for Convolution-Normalzation-Activation blocks.\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input image\n",
        "        out_channels (int): Number of channels produced by the Convolution-Normalzation-Activation block\n",
        "        kernel_size: (int, optional): Size of the convolving kernel. Default: 3\n",
        "        stride (int, optional): Stride of the convolution. Default: 1\n",
        "        padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in wich case it will calculated as ``padding = (kernel_size - 1) // 2 * dilation``\n",
        "        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
        "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolutiuon layer. If ``None`` this layer wont be used. Default: ``torch.nn.BatchNorm2d``\n",
        "        activation_layer (Callable[..., torch.nn.Module], optinal): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer wont be used. Default: ``torch.nn.ReLU``\n",
        "        dilation (int): Spacing between kernel elements. Default: 1\n",
        "        inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True``\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        padding: Optional[int] = None,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm2d,\n",
        "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
        "        dilation: int = 1,\n",
        "        inplace: bool = True,\n",
        "    ) -> None:\n",
        "        if padding is None:\n",
        "            padding = (kernel_size - 1) // 2 * dilation\n",
        "        layers = [\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                dilation=dilation,\n",
        "                groups=groups,\n",
        "                bias=norm_layer is None,\n",
        "            )\n",
        "        ]\n",
        "        if norm_layer is not None:\n",
        "            layers.append(norm_layer(out_channels))\n",
        "        if activation_layer is not None:\n",
        "            layers.append(activation_layer(inplace=inplace))\n",
        "        super().__init__(*layers)\n",
        "        # _log_api_usage_once(self)\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "\n",
        "class SqueezeExcitation(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    This block implements the Squeeze-and-Excitation block from https://arxiv.org/abs/1709.01507 (see Fig. 1).\n",
        "    Parameters ``activation``, and ``scale_activation`` correspond to ``delta`` and ``sigma`` in in eq. 3.\n",
        "    Args:\n",
        "        input_channels (int): Number of channels in the input image\n",
        "        squeeze_channels (int): Number of squeeze channels\n",
        "        activation (Callable[..., torch.nn.Module], optional): ``delta`` activation. Default: ``torch.nn.ReLU``\n",
        "        scale_activation (Callable[..., torch.nn.Module]): ``sigma`` activation. Default: ``torch.nn.Sigmoid``\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels: int,\n",
        "        squeeze_channels: int,\n",
        "        activation: Callable[..., torch.nn.Module] = torch.nn.ReLU,\n",
        "        scale_activation: Callable[..., torch.nn.Module] = torch.nn.Sigmoid,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        # _log_api_usage_once(self)\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = torch.nn.Conv2d(input_channels, squeeze_channels, 1)\n",
        "        self.fc2 = torch.nn.Conv2d(squeeze_channels, input_channels, 1)\n",
        "        self.activation = activation()\n",
        "        self.scale_activation = scale_activation()\n",
        "\n",
        "    def _scale(self, input: Tensor) -> Tensor:\n",
        "        scale = self.avgpool(input)\n",
        "        scale = self.fc1(scale)\n",
        "        scale = self.activation(scale)\n",
        "        scale = self.fc2(scale)\n",
        "        return self.scale_activation(scale)\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        scale = self._scale(input)\n",
        "        return scale * input\n",
        "\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class SimpleStemIN(ConvNormActivation):\n",
        "    \"\"\"Simple stem for ImageNet: 3x3, BN, ReLU.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        width_in: int,\n",
        "        width_out: int,\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        activation_layer: Callable[..., nn.Module],\n",
        "    ) -> None:\n",
        "        super().__init__(width_in, width_out, kernel_size=3, stride=2,\n",
        "                         norm_layer=norm_layer, activation_layer=activation_layer)\n",
        "\n",
        "\n",
        "class BottleneckTransform(nn.Sequential):\n",
        "    \"\"\"Bottleneck transformation: 1x1, 3x3 [+SE], 1x1.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        width_in: int,\n",
        "        width_out: int,\n",
        "        stride: int,\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        activation_layer: Callable[..., nn.Module],\n",
        "        group_width: int,\n",
        "        bottleneck_multiplier: float,\n",
        "        se_ratio: Optional[float],\n",
        "    ) -> None:\n",
        "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
        "        w_b = int(round(width_out * bottleneck_multiplier))\n",
        "        g = w_b // group_width\n",
        "\n",
        "        layers[\"a\"] = ConvNormActivation(width_in, w_b, kernel_size=1, stride=1,\n",
        "                                         norm_layer=norm_layer, activation_layer=activation_layer)\n",
        "        layers[\"b\"] = ConvNormActivation(w_b, w_b, kernel_size=3, stride=stride, groups=g,\n",
        "                                         norm_layer=norm_layer, activation_layer=activation_layer)\n",
        "\n",
        "        if se_ratio:\n",
        "            # The SE reduction ratio is defined with respect to the\n",
        "            # beginning of the block\n",
        "            width_se_out = int(round(se_ratio * width_in))\n",
        "            layers[\"se\"] = SqueezeExcitation(\n",
        "                input_channels=w_b,\n",
        "                squeeze_channels=width_se_out,\n",
        "                activation=activation_layer,\n",
        "            )\n",
        "\n",
        "        layers[\"c\"] = ConvNormActivation(w_b, width_out, kernel_size=1, stride=1,\n",
        "                                         norm_layer=norm_layer, activation_layer=None)\n",
        "        super().__init__(layers)\n",
        "\n",
        "\n",
        "class ResBottleneckBlock(nn.Module):\n",
        "    \"\"\"Residual bottleneck block: x + F(x), F = bottleneck transform.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        width_in: int,\n",
        "        width_out: int,\n",
        "        stride: int,\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        activation_layer: Callable[..., nn.Module],\n",
        "        group_width: int = 1,\n",
        "        bottleneck_multiplier: float = 1.0,\n",
        "        se_ratio: Optional[float] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Use skip connection with projection if shape changes\n",
        "        self.proj = None\n",
        "        should_proj = (width_in != width_out) or (stride != 1)\n",
        "        if should_proj:\n",
        "            self.proj = ConvNormActivation(width_in, width_out, kernel_size=1,\n",
        "                                           stride=stride, norm_layer=norm_layer, activation_layer=None)\n",
        "        self.f = BottleneckTransform(\n",
        "            width_in,\n",
        "            width_out,\n",
        "            stride,\n",
        "            norm_layer,\n",
        "            activation_layer,\n",
        "            group_width,\n",
        "            bottleneck_multiplier,\n",
        "            se_ratio,\n",
        "        )\n",
        "        self.activation = activation_layer(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.proj is not None:\n",
        "            x = self.proj(x) + self.f(x)\n",
        "        else:\n",
        "            x = x + self.f(x)\n",
        "        return self.activation(x)\n",
        "\n",
        "\n",
        "class AnyStage(nn.Sequential):\n",
        "    \"\"\"AnyNet stage (sequence of blocks w/ the same output shape).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        width_in: int,\n",
        "        width_out: int,\n",
        "        stride: int,\n",
        "        depth: int,\n",
        "        block_constructor: Callable[..., nn.Module],\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        activation_layer: Callable[..., nn.Module],\n",
        "        group_width: int,\n",
        "        bottleneck_multiplier: float,\n",
        "        se_ratio: Optional[float] = None,\n",
        "        stage_index: int = 0,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        for i in range(depth):\n",
        "            block = block_constructor(\n",
        "                width_in if i == 0 else width_out,\n",
        "                width_out,\n",
        "                stride if i == 0 else 1,\n",
        "                norm_layer,\n",
        "                activation_layer,\n",
        "                group_width,\n",
        "                bottleneck_multiplier,\n",
        "                se_ratio,\n",
        "            )\n",
        "\n",
        "            self.add_module(f\"block{stage_index}-{i}\", block)\n",
        "\n",
        "\n",
        "class BlockParams:\n",
        "    def __init__(\n",
        "        self,\n",
        "        depths: List[int],\n",
        "        widths: List[int],\n",
        "        group_widths: List[int],\n",
        "        bottleneck_multipliers: List[float],\n",
        "        strides: List[int],\n",
        "        se_ratio: Optional[float] = None,\n",
        "    ) -> None:\n",
        "        self.depths = depths\n",
        "        self.widths = widths\n",
        "        self.group_widths = group_widths\n",
        "        self.bottleneck_multipliers = bottleneck_multipliers\n",
        "        self.strides = strides\n",
        "        self.se_ratio = se_ratio\n",
        "\n",
        "    @classmethod\n",
        "    def from_init_params(\n",
        "        cls,\n",
        "        depth: int,\n",
        "        w_0: int,\n",
        "        w_a: float,\n",
        "        w_m: float,\n",
        "        group_width: int,\n",
        "        bottleneck_multiplier: float = 1.0,\n",
        "        se_ratio: Optional[float] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> \"BlockParams\":\n",
        "        \"\"\"\n",
        "        Programatically compute all the per-block settings,\n",
        "        given the RegNet parameters.\n",
        "\n",
        "        The first step is to compute the quantized linear block parameters,\n",
        "        in log space. Key parameters are:\n",
        "        - `w_a` is the width progression slope\n",
        "        - `w_0` is the initial width\n",
        "        - `w_m` is the width stepping in the log space\n",
        "\n",
        "        In other terms\n",
        "        `log(block_width) = log(w_0) + w_m * block_capacity`,\n",
        "        with `bock_capacity` ramping up following the w_0 and w_a params.\n",
        "        This block width is finally quantized to multiples of 8.\n",
        "\n",
        "        The second step is to compute the parameters per stage,\n",
        "        taking into account the skip connection and the final 1x1 convolutions.\n",
        "        We use the fact that the output width is constant within a stage.\n",
        "        \"\"\"\n",
        "\n",
        "        QUANT = 8\n",
        "        STRIDE = 2\n",
        "\n",
        "        if w_a < 0 or w_0 <= 0 or w_m <= 1 or w_0 % 8 != 0:\n",
        "            raise ValueError(\"Invalid RegNet settings\")\n",
        "        # Compute the block widths. Each stage has one unique block width\n",
        "        widths_cont = torch.arange(depth) * w_a + w_0\n",
        "        block_capacity = torch.round(torch.log(widths_cont / w_0) / math.log(w_m))\n",
        "        block_widths = (\n",
        "            torch.round(torch.divide(w_0 * torch.pow(w_m, block_capacity), QUANT))\n",
        "            * QUANT\n",
        "        ).int().tolist()\n",
        "        num_stages = len(set(block_widths))\n",
        "\n",
        "        # Convert to per stage parameters\n",
        "        split_helper = zip(\n",
        "            block_widths + [0],\n",
        "            [0] + block_widths,\n",
        "            block_widths + [0],\n",
        "            [0] + block_widths,\n",
        "        )\n",
        "        splits = [w != wp or r != rp for w, wp, r, rp in split_helper]\n",
        "\n",
        "        stage_widths = [w for w, t in zip(block_widths, splits[:-1]) if t]\n",
        "        stage_depths = torch.diff(torch.tensor([d for d, t in enumerate(splits) if t])).int().tolist()\n",
        "\n",
        "        strides = [STRIDE] * num_stages\n",
        "        bottleneck_multipliers = [bottleneck_multiplier] * num_stages\n",
        "        group_widths = [group_width] * num_stages\n",
        "\n",
        "        # Adjust the compatibility of stage widths and group widths\n",
        "        stage_widths, group_widths = cls._adjust_widths_groups_compatibilty(\n",
        "            stage_widths, bottleneck_multipliers, group_widths\n",
        "        )\n",
        "\n",
        "        return cls(\n",
        "            depths=stage_depths,\n",
        "            widths=stage_widths,\n",
        "            group_widths=group_widths,\n",
        "            bottleneck_multipliers=bottleneck_multipliers,\n",
        "            strides=strides,\n",
        "            se_ratio=se_ratio,\n",
        "        )\n",
        "\n",
        "    def _get_expanded_params(self):\n",
        "        return zip(\n",
        "            self.widths, self.strides, self.depths, self.group_widths, self.bottleneck_multipliers\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _adjust_widths_groups_compatibilty(\n",
        "            stage_widths: List[int], bottleneck_ratios: List[float],\n",
        "            group_widths: List[int]) -> Tuple[List[int], List[int]]:\n",
        "        \"\"\"\n",
        "        Adjusts the compatibility of widths and groups,\n",
        "        depending on the bottleneck ratio.\n",
        "        \"\"\"\n",
        "        # Compute all widths for the current settings\n",
        "        widths = [int(w * b) for w, b in zip(stage_widths, bottleneck_ratios)]\n",
        "        group_widths_min = [min(g, w_bot) for g, w_bot in zip(group_widths, widths)]\n",
        "\n",
        "        # Compute the adjusted widths so that stage and group widths fit\n",
        "        ws_bot = [_make_divisible(w_bot, g) for w_bot, g in zip(widths, group_widths_min)]\n",
        "        stage_widths = [int(w_bot / b) for w_bot, b in zip(ws_bot, bottleneck_ratios)]\n",
        "        return stage_widths, group_widths_min\n",
        "\n",
        "\n",
        "class RegNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block_params: BlockParams,\n",
        "        in_channels: int = 3,\n",
        "        num_classes: int = 1000,\n",
        "        stem_width: int = 32,\n",
        "        stem_type: Optional[Callable[..., nn.Module]] = None,\n",
        "        block_type: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        activation: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if stem_type is None:\n",
        "            stem_type = SimpleStemIN\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if block_type is None:\n",
        "            block_type = ResBottleneckBlock\n",
        "        if activation is None:\n",
        "            activation = nn.ReLU\n",
        "\n",
        "        # Ad hoc stem\n",
        "        self.stem = stem_type(\n",
        "            in_channels,  # width_in\n",
        "            stem_width,\n",
        "            norm_layer,\n",
        "            activation,\n",
        "        )\n",
        "\n",
        "        current_width = stem_width\n",
        "\n",
        "        blocks = []\n",
        "        for i, (\n",
        "            width_out,\n",
        "            stride,\n",
        "            depth,\n",
        "            group_width,\n",
        "            bottleneck_multiplier,\n",
        "        ) in enumerate(block_params._get_expanded_params()):\n",
        "            blocks.append(\n",
        "                (\n",
        "                    f\"block{i+1}\",\n",
        "                    AnyStage(\n",
        "                        current_width,\n",
        "                        width_out,\n",
        "                        stride,\n",
        "                        depth,\n",
        "                        block_type,\n",
        "                        norm_layer,\n",
        "                        activation,\n",
        "                        group_width,\n",
        "                        bottleneck_multiplier,\n",
        "                        block_params.se_ratio,\n",
        "                        stage_index=i + 1,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            current_width = width_out\n",
        "\n",
        "        self.trunk_output = nn.Sequential(OrderedDict(blocks))\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(in_features=current_width, out_features=num_classes)\n",
        "\n",
        "        # Init weights and good to go\n",
        "        self._reset_parameters()\n",
        "\n",
        "        # initial weights\n",
        "        for m in self.modules():\n",
        "          if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "          elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "          elif isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, 0, 0.01)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.stem(x)\n",
        "        x = self.trunk_output(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _reset_parameters(self) -> None:\n",
        "        # Performs ResNet-style weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # Note that there is no bias due to BN\n",
        "                fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=math.sqrt(2.0 / fan_out))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "def _regnet(arch: str, block_params: BlockParams, in_channels, num_classes, pretrained: bool, progress: bool, **kwargs: Any) -> RegNet:\n",
        "    model = RegNet(block_params, in_channels=in_channels, num_classes=num_classes, norm_layer=partial(nn.BatchNorm2d, eps=1e-05, momentum=0.1), **kwargs)\n",
        "    if pretrained:\n",
        "        if arch not in model_urls:\n",
        "            raise ValueError(f\"No checkpoint is available for model type {arch}\")\n",
        "        # state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
        "        # model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "def regnet_y_400mf(in_channels, num_classes, pretrained: bool = False, progress: bool = True, **kwargs: Any) -> RegNet:\n",
        "    \"\"\"\n",
        "    Constructs a RegNetY_400MF architecture from\n",
        "    `\"Designing Network Design Spaces\" <https://arxiv.org/abs/2003.13678>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    params = BlockParams.from_init_params(depth=16, w_0=48, w_a=27.89, w_m=2.09,\n",
        "                                          group_width=8, se_ratio=0.25, **kwargs)\n",
        "    return _regnet(\"regnet_y_400mf\", params, in_channels, num_classes, pretrained, progress, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3kbzpQsuQbOA",
      "metadata": {
        "id": "3kbzpQsuQbOA"
      },
      "outputs": [],
      "source": [
        "class ClassificationNetwork(torch.nn.Module):\n",
        "    def __init__(self, in_channels, embedding, num_classes):\n",
        "        super().__init__()\n",
        "        self.cnn = regnet_y_400mf(in_channels, embedding)\n",
        "        self.mlp = nn.Sequential(nn.Linear(embedding,num_classes))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.cnn(x)\n",
        "      out = self.mlp(out)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "964a4c9e",
      "metadata": {
        "id": "964a4c9e"
      },
      "outputs": [],
      "source": [
        "numEpochs = 100\n",
        "in_features = 15 # TODO: change RGB channels according to num of frames\n",
        "embedding = 512\n",
        "\n",
        "learningRate = 0.1\n",
        "weightDecay = 1e-4\n",
        "\n",
        "num_classes = 120\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "network = ClassificationNetwork(in_features, embedding, num_classes)\n",
        "network = network.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2TdiwHBaNFvt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TdiwHBaNFvt",
        "outputId": "f1a2a8ac-f7c2-456d-b067-a73503a888db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11\tBatch: 50\tAvg-Loss: 2.0942\tTraining Accuracy : 0.2706\n",
            "Epoch: 11\tBatch: 100\tAvg-Loss: 2.1120\tTraining Accuracy : 0.2677\n",
            "Epoch: 11\tBatch: 150\tAvg-Loss: 2.1366\tTraining Accuracy : 0.2581\n",
            "Epoch: 11\tBatch: 200\tAvg-Loss: 2.1323\tTraining Accuracy : 0.2700\n",
            "Epoch: 11\tBatch: 250\tAvg-Loss: 2.1518\tTraining Accuracy : 0.2639\n",
            "Epoch: 11\tBatch: 300\tAvg-Loss: 2.1316\tTraining Accuracy : 0.2611\n",
            "Epoch: 11\tBatch: 350\tAvg-Loss: 2.1328\tTraining Accuracy : 0.2723\n",
            "Epoch: 11\tBatch: 400\tAvg-Loss: 2.1386\tTraining Accuracy : 0.2722\n",
            "Epoch: 11\tBatch: 450\tAvg-Loss: 2.1472\tTraining Accuracy : 0.2630\n",
            "Epoch: 11\tBatch: 500\tAvg-Loss: 2.1445\tTraining Accuracy : 0.2634\n",
            "Epoch: 11\tBatch: 550\tAvg-Loss: 2.1329\tTraining Accuracy : 0.2692\n",
            "Epoch: 11\tBatch: 600\tAvg-Loss: 2.1495\tTraining Accuracy : 0.2575\n",
            "Epoch: 11\tBatch: 650\tAvg-Loss: 2.1438\tTraining Accuracy : 0.2622\n",
            "Epoch: 11, Validation Loss: 2.380, Validation Accuracy: 0.238\n",
            "Epoch: 12\tBatch: 50\tAvg-Loss: 2.1070\tTraining Accuracy : 0.2709\n",
            "Epoch: 12\tBatch: 100\tAvg-Loss: 2.0844\tTraining Accuracy : 0.2752\n",
            "Epoch: 12\tBatch: 150\tAvg-Loss: 2.0831\tTraining Accuracy : 0.2778\n",
            "Epoch: 12\tBatch: 200\tAvg-Loss: 2.0954\tTraining Accuracy : 0.2727\n",
            "Epoch: 12\tBatch: 250\tAvg-Loss: 2.0909\tTraining Accuracy : 0.2703\n",
            "Epoch: 12\tBatch: 300\tAvg-Loss: 2.0918\tTraining Accuracy : 0.2712\n",
            "Epoch: 12\tBatch: 350\tAvg-Loss: 2.0901\tTraining Accuracy : 0.2712\n",
            "Epoch: 12\tBatch: 400\tAvg-Loss: 2.1283\tTraining Accuracy : 0.2694\n",
            "Epoch: 12\tBatch: 450\tAvg-Loss: 2.1082\tTraining Accuracy : 0.2658\n",
            "Epoch: 12\tBatch: 500\tAvg-Loss: 2.1050\tTraining Accuracy : 0.2728\n",
            "Epoch: 12\tBatch: 550\tAvg-Loss: 2.1060\tTraining Accuracy : 0.2609\n",
            "Epoch: 12\tBatch: 600\tAvg-Loss: 2.0957\tTraining Accuracy : 0.2633\n",
            "Epoch: 12\tBatch: 650\tAvg-Loss: 2.0753\tTraining Accuracy : 0.2816\n",
            "Epoch: 12, Validation Loss: 2.296, Validation Accuracy: 0.245\n",
            "Epoch: 13\tBatch: 50\tAvg-Loss: 2.0214\tTraining Accuracy : 0.2906\n",
            "Epoch: 13\tBatch: 100\tAvg-Loss: 2.0207\tTraining Accuracy : 0.2819\n",
            "Epoch: 13\tBatch: 150\tAvg-Loss: 2.0182\tTraining Accuracy : 0.2880\n",
            "Epoch: 13\tBatch: 200\tAvg-Loss: 2.0484\tTraining Accuracy : 0.2809\n",
            "Epoch: 13\tBatch: 250\tAvg-Loss: 2.0318\tTraining Accuracy : 0.2709\n",
            "Epoch: 13\tBatch: 300\tAvg-Loss: 2.0412\tTraining Accuracy : 0.2800\n",
            "Epoch: 13\tBatch: 350\tAvg-Loss: 2.0507\tTraining Accuracy : 0.2841\n",
            "Epoch: 13\tBatch: 400\tAvg-Loss: 2.0444\tTraining Accuracy : 0.2792\n",
            "Epoch: 13\tBatch: 450\tAvg-Loss: 2.0607\tTraining Accuracy : 0.2816\n",
            "Epoch: 13\tBatch: 500\tAvg-Loss: 2.0400\tTraining Accuracy : 0.2711\n",
            "Epoch: 13\tBatch: 550\tAvg-Loss: 2.0260\tTraining Accuracy : 0.2831\n",
            "Epoch: 13\tBatch: 600\tAvg-Loss: 2.0130\tTraining Accuracy : 0.2798\n",
            "Epoch: 13\tBatch: 650\tAvg-Loss: 2.0148\tTraining Accuracy : 0.2839\n",
            "Epoch: 13, Validation Loss: 2.379, Validation Accuracy: 0.243\n",
            "Epoch: 14\tBatch: 50\tAvg-Loss: 1.9386\tTraining Accuracy : 0.2948\n",
            "Epoch: 14\tBatch: 100\tAvg-Loss: 1.9287\tTraining Accuracy : 0.2906\n",
            "Epoch: 14\tBatch: 150\tAvg-Loss: 1.9572\tTraining Accuracy : 0.2916\n",
            "Epoch: 14\tBatch: 200\tAvg-Loss: 1.9929\tTraining Accuracy : 0.2942\n",
            "Epoch: 14\tBatch: 250\tAvg-Loss: 1.9861\tTraining Accuracy : 0.2902\n",
            "Epoch: 14\tBatch: 300\tAvg-Loss: 1.9795\tTraining Accuracy : 0.2841\n",
            "Epoch: 14\tBatch: 350\tAvg-Loss: 1.9541\tTraining Accuracy : 0.2980\n",
            "Epoch: 14\tBatch: 400\tAvg-Loss: 1.9516\tTraining Accuracy : 0.2952\n",
            "Epoch: 14\tBatch: 450\tAvg-Loss: 1.9727\tTraining Accuracy : 0.2827\n",
            "Epoch: 14\tBatch: 500\tAvg-Loss: 1.9618\tTraining Accuracy : 0.2891\n",
            "Epoch: 14\tBatch: 550\tAvg-Loss: 1.9727\tTraining Accuracy : 0.2867\n",
            "Epoch: 14\tBatch: 600\tAvg-Loss: 1.9753\tTraining Accuracy : 0.2919\n",
            "Epoch: 14\tBatch: 650\tAvg-Loss: 1.9441\tTraining Accuracy : 0.2986\n",
            "Epoch: 14, Validation Loss: 2.394, Validation Accuracy: 0.239\n",
            "Epoch: 15\tBatch: 50\tAvg-Loss: 1.8766\tTraining Accuracy : 0.3114\n",
            "Epoch: 15\tBatch: 100\tAvg-Loss: 1.8809\tTraining Accuracy : 0.3055\n",
            "Epoch: 15\tBatch: 150\tAvg-Loss: 1.8815\tTraining Accuracy : 0.3108\n",
            "Epoch: 15\tBatch: 200\tAvg-Loss: 1.8610\tTraining Accuracy : 0.3063\n",
            "Epoch: 15\tBatch: 250\tAvg-Loss: 1.8830\tTraining Accuracy : 0.3102\n",
            "Epoch: 15\tBatch: 300\tAvg-Loss: 1.8909\tTraining Accuracy : 0.3097\n",
            "Epoch: 15\tBatch: 350\tAvg-Loss: 1.8726\tTraining Accuracy : 0.3166\n",
            "Epoch: 15\tBatch: 400\tAvg-Loss: 1.8789\tTraining Accuracy : 0.3089\n",
            "Epoch: 15\tBatch: 450\tAvg-Loss: 1.9053\tTraining Accuracy : 0.3017\n",
            "Epoch: 15\tBatch: 500\tAvg-Loss: 1.8896\tTraining Accuracy : 0.3027\n",
            "Epoch: 15\tBatch: 550\tAvg-Loss: 1.8641\tTraining Accuracy : 0.3228\n",
            "Epoch: 15\tBatch: 600\tAvg-Loss: 1.8991\tTraining Accuracy : 0.2958\n",
            "Epoch: 15\tBatch: 650\tAvg-Loss: 1.8600\tTraining Accuracy : 0.3089\n",
            "Epoch     5: reducing learning rate of group 0 to 5.0000e-02.\n",
            "Epoch: 15, Validation Loss: 2.610, Validation Accuracy: 0.236\n",
            "Epoch: 16\tBatch: 50\tAvg-Loss: 1.6590\tTraining Accuracy : 0.3639\n",
            "Epoch: 16\tBatch: 100\tAvg-Loss: 1.6228\tTraining Accuracy : 0.3727\n",
            "Epoch: 16\tBatch: 150\tAvg-Loss: 1.6122\tTraining Accuracy : 0.3628\n",
            "Epoch: 16\tBatch: 200\tAvg-Loss: 1.6044\tTraining Accuracy : 0.3616\n",
            "Epoch: 16\tBatch: 250\tAvg-Loss: 1.5949\tTraining Accuracy : 0.3606\n",
            "Epoch: 16\tBatch: 300\tAvg-Loss: 1.5886\tTraining Accuracy : 0.3767\n",
            "Epoch: 16\tBatch: 350\tAvg-Loss: 1.5963\tTraining Accuracy : 0.3742\n",
            "Epoch: 16\tBatch: 400\tAvg-Loss: 1.6150\tTraining Accuracy : 0.3625\n",
            "Epoch: 16\tBatch: 450\tAvg-Loss: 1.5772\tTraining Accuracy : 0.3833\n",
            "Epoch: 16\tBatch: 500\tAvg-Loss: 1.5860\tTraining Accuracy : 0.3861\n",
            "Epoch: 16\tBatch: 550\tAvg-Loss: 1.6138\tTraining Accuracy : 0.3839\n",
            "Epoch: 16\tBatch: 600\tAvg-Loss: 1.5869\tTraining Accuracy : 0.3808\n",
            "Epoch: 16\tBatch: 650\tAvg-Loss: 1.5795\tTraining Accuracy : 0.3892\n",
            "Epoch: 16, Validation Loss: 2.887, Validation Accuracy: 0.255\n",
            "Epoch: 17\tBatch: 50\tAvg-Loss: 1.4208\tTraining Accuracy : 0.4323\n",
            "Epoch: 17\tBatch: 100\tAvg-Loss: 1.4906\tTraining Accuracy : 0.4156\n",
            "Epoch: 17\tBatch: 150\tAvg-Loss: 1.4774\tTraining Accuracy : 0.4164\n",
            "Epoch: 17\tBatch: 200\tAvg-Loss: 1.4959\tTraining Accuracy : 0.4175\n",
            "Epoch: 17\tBatch: 250\tAvg-Loss: 1.5046\tTraining Accuracy : 0.4148\n",
            "Epoch: 17\tBatch: 300\tAvg-Loss: 1.5126\tTraining Accuracy : 0.4208\n",
            "Epoch: 17\tBatch: 350\tAvg-Loss: 1.5495\tTraining Accuracy : 0.4153\n",
            "Epoch: 17\tBatch: 400\tAvg-Loss: 1.5112\tTraining Accuracy : 0.4222\n",
            "Epoch: 17\tBatch: 450\tAvg-Loss: 1.4847\tTraining Accuracy : 0.4258\n",
            "Epoch: 17\tBatch: 500\tAvg-Loss: 1.4964\tTraining Accuracy : 0.4269\n",
            "Epoch: 17\tBatch: 550\tAvg-Loss: 1.5161\tTraining Accuracy : 0.4203\n",
            "Epoch: 17\tBatch: 600\tAvg-Loss: 1.4844\tTraining Accuracy : 0.4375\n",
            "Epoch: 17\tBatch: 650\tAvg-Loss: 1.5114\tTraining Accuracy : 0.4378\n",
            "Epoch: 17, Validation Loss: 2.840, Validation Accuracy: 0.246\n",
            "Epoch: 18\tBatch: 50\tAvg-Loss: 1.3755\tTraining Accuracy : 0.4827\n",
            "Epoch: 18\tBatch: 100\tAvg-Loss: 1.3951\tTraining Accuracy : 0.4838\n",
            "Epoch: 18\tBatch: 150\tAvg-Loss: 1.3862\tTraining Accuracy : 0.4908\n",
            "Epoch: 18\tBatch: 200\tAvg-Loss: 1.4161\tTraining Accuracy : 0.4748\n",
            "Epoch: 18\tBatch: 250\tAvg-Loss: 1.3912\tTraining Accuracy : 0.4859\n",
            "Epoch: 18\tBatch: 300\tAvg-Loss: 1.4025\tTraining Accuracy : 0.4900\n",
            "Epoch: 18\tBatch: 350\tAvg-Loss: 1.3847\tTraining Accuracy : 0.5038\n",
            "Epoch: 18\tBatch: 400\tAvg-Loss: 1.4069\tTraining Accuracy : 0.4994\n",
            "Epoch: 18\tBatch: 450\tAvg-Loss: 1.3771\tTraining Accuracy : 0.5133\n",
            "Epoch: 18\tBatch: 500\tAvg-Loss: 1.4218\tTraining Accuracy : 0.4973\n",
            "Epoch: 18\tBatch: 550\tAvg-Loss: 1.3446\tTraining Accuracy : 0.5209\n",
            "Epoch: 18\tBatch: 600\tAvg-Loss: 1.3548\tTraining Accuracy : 0.5288\n",
            "Epoch: 18\tBatch: 650\tAvg-Loss: 1.3097\tTraining Accuracy : 0.5448\n",
            "Epoch     8: reducing learning rate of group 0 to 2.5000e-02.\n",
            "Epoch: 18, Validation Loss: 2.924, Validation Accuracy: 0.224\n",
            "Epoch: 19\tBatch: 50\tAvg-Loss: 1.0991\tTraining Accuracy : 0.6209\n",
            "Epoch: 19\tBatch: 100\tAvg-Loss: 0.9743\tTraining Accuracy : 0.6661\n",
            "Epoch: 19\tBatch: 150\tAvg-Loss: 0.9699\tTraining Accuracy : 0.6709\n",
            "Epoch: 19\tBatch: 200\tAvg-Loss: 0.9739\tTraining Accuracy : 0.6613\n",
            "Epoch: 19\tBatch: 250\tAvg-Loss: 0.9643\tTraining Accuracy : 0.6669\n",
            "Epoch: 19\tBatch: 300\tAvg-Loss: 0.9780\tTraining Accuracy : 0.6637\n",
            "Epoch: 19\tBatch: 350\tAvg-Loss: 0.9540\tTraining Accuracy : 0.6770\n",
            "Epoch: 19\tBatch: 400\tAvg-Loss: 0.9469\tTraining Accuracy : 0.6780\n",
            "Epoch: 19\tBatch: 450\tAvg-Loss: 0.9487\tTraining Accuracy : 0.6808\n",
            "Epoch: 19\tBatch: 500\tAvg-Loss: 0.9400\tTraining Accuracy : 0.6758\n",
            "Epoch: 19\tBatch: 550\tAvg-Loss: 0.9454\tTraining Accuracy : 0.6767\n",
            "Epoch: 19\tBatch: 600\tAvg-Loss: 0.9370\tTraining Accuracy : 0.6855\n",
            "Epoch: 19\tBatch: 650\tAvg-Loss: 0.8893\tTraining Accuracy : 0.6952\n",
            "Epoch: 19, Validation Loss: 3.614, Validation Accuracy: 0.241\n",
            "Epoch: 20\tBatch: 50\tAvg-Loss: 0.7725\tTraining Accuracy : 0.7383\n",
            "Epoch: 20\tBatch: 100\tAvg-Loss: 0.7702\tTraining Accuracy : 0.7417\n",
            "Epoch: 20\tBatch: 150\tAvg-Loss: 0.7530\tTraining Accuracy : 0.7406\n",
            "Epoch: 20\tBatch: 200\tAvg-Loss: 0.7334\tTraining Accuracy : 0.7523\n",
            "Epoch: 20\tBatch: 250\tAvg-Loss: 0.7998\tTraining Accuracy : 0.7231\n",
            "Epoch: 20\tBatch: 300\tAvg-Loss: 0.7874\tTraining Accuracy : 0.7314\n",
            "Epoch: 20\tBatch: 350\tAvg-Loss: 0.8104\tTraining Accuracy : 0.7289\n",
            "Epoch: 20\tBatch: 400\tAvg-Loss: 0.8079\tTraining Accuracy : 0.7178\n",
            "Epoch: 20\tBatch: 450\tAvg-Loss: 0.8371\tTraining Accuracy : 0.7173\n",
            "Epoch: 20\tBatch: 500\tAvg-Loss: 0.7967\tTraining Accuracy : 0.7348\n",
            "Epoch: 20\tBatch: 550\tAvg-Loss: 0.8349\tTraining Accuracy : 0.7147\n",
            "Epoch: 20\tBatch: 600\tAvg-Loss: 0.8230\tTraining Accuracy : 0.7222\n",
            "Epoch: 20\tBatch: 650\tAvg-Loss: 0.8168\tTraining Accuracy : 0.7283\n",
            "Epoch: 20, Validation Loss: 3.770, Validation Accuracy: 0.242\n",
            "Epoch: 21\tBatch: 50\tAvg-Loss: 0.7022\tTraining Accuracy : 0.7605\n",
            "Epoch: 21\tBatch: 100\tAvg-Loss: 0.6679\tTraining Accuracy : 0.7762\n",
            "Epoch: 21\tBatch: 150\tAvg-Loss: 0.6678\tTraining Accuracy : 0.7730\n",
            "Epoch: 21\tBatch: 200\tAvg-Loss: 0.6824\tTraining Accuracy : 0.7723\n",
            "Epoch: 21\tBatch: 250\tAvg-Loss: 0.6941\tTraining Accuracy : 0.7620\n",
            "Epoch: 21\tBatch: 300\tAvg-Loss: 0.6971\tTraining Accuracy : 0.7616\n",
            "Epoch: 21\tBatch: 350\tAvg-Loss: 0.7302\tTraining Accuracy : 0.7516\n",
            "Epoch: 21\tBatch: 400\tAvg-Loss: 0.7377\tTraining Accuracy : 0.7512\n",
            "Epoch: 21\tBatch: 450\tAvg-Loss: 0.7216\tTraining Accuracy : 0.7558\n",
            "Epoch: 21\tBatch: 500\tAvg-Loss: 0.7323\tTraining Accuracy : 0.7475\n",
            "Epoch: 21\tBatch: 550\tAvg-Loss: 0.7418\tTraining Accuracy : 0.7441\n",
            "Epoch: 21\tBatch: 600\tAvg-Loss: 0.7600\tTraining Accuracy : 0.7377\n",
            "Epoch: 21\tBatch: 650\tAvg-Loss: 0.7404\tTraining Accuracy : 0.7438\n",
            "Epoch    11: reducing learning rate of group 0 to 1.2500e-02.\n",
            "Epoch: 21, Validation Loss: 3.847, Validation Accuracy: 0.231\n",
            "Epoch: 22\tBatch: 50\tAvg-Loss: 0.5914\tTraining Accuracy : 0.8034\n",
            "Epoch: 22\tBatch: 100\tAvg-Loss: 0.4993\tTraining Accuracy : 0.8358\n",
            "Epoch: 22\tBatch: 150\tAvg-Loss: 0.4880\tTraining Accuracy : 0.8377\n",
            "Epoch: 22\tBatch: 200\tAvg-Loss: 0.4995\tTraining Accuracy : 0.8316\n",
            "Epoch: 22\tBatch: 250\tAvg-Loss: 0.4822\tTraining Accuracy : 0.8387\n",
            "Epoch: 22\tBatch: 300\tAvg-Loss: 0.4885\tTraining Accuracy : 0.8363\n",
            "Epoch: 22\tBatch: 350\tAvg-Loss: 0.4796\tTraining Accuracy : 0.8369\n",
            "Epoch: 22\tBatch: 400\tAvg-Loss: 0.4885\tTraining Accuracy : 0.8328\n",
            "Epoch: 22\tBatch: 450\tAvg-Loss: 0.4999\tTraining Accuracy : 0.8331\n",
            "Epoch: 22\tBatch: 500\tAvg-Loss: 0.4735\tTraining Accuracy : 0.8400\n",
            "Epoch: 22\tBatch: 550\tAvg-Loss: 0.4821\tTraining Accuracy : 0.8309\n"
          ]
        }
      ],
      "source": [
        "# Train!\n",
        "max_val_acc = 0\n",
        "for epoch in range(11, numEpochs):\n",
        "    # Train\n",
        "    network.train()\n",
        "    avg_loss = 0.0\n",
        "    avg_train_acc = 0.0\n",
        "    for batch_num, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x.requires_grad_()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = network(x)\n",
        "        num_train_correct = (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "        num_labels = len(y)\n",
        "        avg_train_acc += (num_train_correct/num_labels)\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "        if batch_num % 50 == 49:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}\\tTraining Accuracy : {:.4f}'.format(epoch, batch_num+1, avg_loss/50, avg_train_acc/50))\n",
        "            avg_loss = 0.0\n",
        "            avg_train_acc = 0.0\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        del x\n",
        "        del y\n",
        "        del loss\n",
        "    \n",
        "    # Validate\n",
        "    with torch.no_grad():\n",
        "      network.eval()\n",
        "      avg_val_loss = 0.0\n",
        "      num_correct = 0\n",
        "      for batch_num, (x, y) in enumerate(val_loader):\n",
        "          x, y = x.to(device), y.to(device)\n",
        "          outputs = network(x)\n",
        "          num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "          loss = criterion(outputs, y.long())\n",
        "          avg_val_loss += loss.item()\n",
        "\n",
        "          torch.cuda.empty_cache()\n",
        "          del x\n",
        "          del y\n",
        "\n",
        "      avg_val_loss = avg_val_loss / len(val_loader)\n",
        "      val_acc = num_correct / len(val_set)\n",
        "      # checkpoint_name = \"/content/gdrive/MyDrive/dl_project/model_checkpoints/efficientnet2/model_\" + str(epoch) + \".pt\"\n",
        "      # torch.save(network.state_dict(), checkpoint_name)\n",
        "      if val_acc > max_val_acc:\n",
        "          max_val_acc = val_acc\n",
        "          torch.save(network.state_dict(), \"/content/gdrive/MyDrive/regnet_ours_data4/best_model_\" + str(epoch) + \".pt\")\n",
        "    scheduler.step(avg_val_loss)\n",
        "    print('Epoch: {}, Validation Loss: {:.3f}, Validation Accuracy: {:.3f}'.format(epoch, avg_val_loss, val_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "SHZTDySnc_80",
      "metadata": {
        "id": "SHZTDySnc_80"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "V3ic11NHefDa",
      "metadata": {
        "id": "V3ic11NHefDa"
      },
      "outputs": [],
      "source": [
        "network = ClassificationNetwork(in_features, embedding, num_classes)\n",
        "network.load_state_dict(torch.load(\"/content/gdrive/MyDrive/regnet_ours_data4/best_model_16.pt\"))\n",
        "network = network.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "t7XRDDeQddip",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7XRDDeQddip",
        "outputId": "48ba3216-95cc-4d90-9f4f-77f14e141b97"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [07:37<00:00,  3.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.22967626901431995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "network.eval()\n",
        "num_correct = 0\n",
        "for x, y in tqdm(test_loader):\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  outputs = network(x)\n",
        "  num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "test_acc = num_correct / len(test_set)\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "EGwrkvRtc6ki",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGwrkvRtc6ki",
        "outputId": "0f0b434c-5e72-45ab-d33b-3164a2856708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.22967626901431995\n"
          ]
        }
      ],
      "source": [
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g9jhAiCjeMno",
      "metadata": {
        "id": "g9jhAiCjeMno"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "RegNet_15channels_direct_DATA4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
