{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7badc3cb",
      "metadata": {
        "id": "7badc3cb"
      },
      "source": [
        "## 1. Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "AuO32Xm2fb1z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuO32Xm2fb1z",
        "outputId": "3688922e-459f-4185-80b4-5c3b0938b9a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "jIlv-QiNgNt7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIlv-QiNgNt7",
        "outputId": "81c8c44d-03af-4e1d-f7c3-e601bee3d786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!cp /content/gdrive/MyDrive/converted_224x224.tar.gz /content\n",
        "%cd /content\n",
        "!tar -xzf converted_224x224.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "812ec14e",
      "metadata": {
        "id": "812ec14e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pdb\n",
        "import gc\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision \n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "# from torchinfo import summary\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c5972057",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5972057",
        "outputId": "1e1a26b5-6953-4a14-9436-67376ce064ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda =  True  with num_workers =  8  system version =  3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ],
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "num_workers = 8 if cuda else 0\n",
        "\n",
        "print(\"Cuda = \", str(cuda), \" with num_workers = \", str(num_workers),  \" system version = \", sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71e53e8",
      "metadata": {
        "id": "e71e53e8"
      },
      "source": [
        "## 2. Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bdee1e8",
      "metadata": {
        "id": "3bdee1e8"
      },
      "source": [
        "### 2.1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6e874ee1",
      "metadata": {
        "id": "6e874ee1"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "df = pd.read_csv(\"4+1.csv\")\n",
        "df = df.iloc[:, 1:]\n",
        "\n",
        "train_end = int(len(df)*0.7)\n",
        "val_end = int(len(df)*0.85)\n",
        "train_data = df[:train_end]\n",
        "val_data = df[train_end:val_end]\n",
        "test_data = df[val_end:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e669450b",
      "metadata": {
        "id": "e669450b"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.reset_index().drop('Unnamed: 0.1',1).drop('index',1)\n",
        "val_data = val_data.reset_index().drop('Unnamed: 0.1',1).drop('index',1)\n",
        "test_data = test_data.reset_index().drop('Unnamed: 0.1',1).drop('index',1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "rGYT1RmDl-fG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "rGYT1RmDl-fG",
        "outputId": "094ece10-edc0-4aa2-d64f-c2ce38c8c645"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>frames</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>('frame_149670.jpg', 'frame_149739.jpg', 'fram...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>('frame_149738.jpg', 'frame_149692.jpg', 'fram...</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>('frame_149729.jpg', 'frame_149716.jpg', 'fram...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>('frame_149669.jpg', 'frame_149712.jpg', 'fram...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>('frame_149683.jpg', 'frame_149707.jpg', 'fram...</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17942</th>\n",
              "      <td>('frame_180809.jpg', 'frame_174579.jpg', 'fram...</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17943</th>\n",
              "      <td>('frame_174581.jpg', 'frame_174634.jpg', 'fram...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17944</th>\n",
              "      <td>('frame_174605.jpg', 'frame_174596.jpg', 'fram...</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17945</th>\n",
              "      <td>('frame_181123.jpg', 'frame_174585.jpg', 'fram...</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17946</th>\n",
              "      <td>('frame_181333.jpg', 'frame_174579.jpg', 'fram...</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17947 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  frames  label\n",
              "0      ('frame_149670.jpg', 'frame_149739.jpg', 'fram...     16\n",
              "1      ('frame_149738.jpg', 'frame_149692.jpg', 'fram...     80\n",
              "2      ('frame_149729.jpg', 'frame_149716.jpg', 'fram...     86\n",
              "3      ('frame_149669.jpg', 'frame_149712.jpg', 'fram...      7\n",
              "4      ('frame_149683.jpg', 'frame_149707.jpg', 'fram...     34\n",
              "...                                                  ...    ...\n",
              "17942  ('frame_180809.jpg', 'frame_174579.jpg', 'fram...    101\n",
              "17943  ('frame_174581.jpg', 'frame_174634.jpg', 'fram...     14\n",
              "17944  ('frame_174605.jpg', 'frame_174596.jpg', 'fram...     54\n",
              "17945  ('frame_181123.jpg', 'frame_174585.jpg', 'fram...     96\n",
              "17946  ('frame_181333.jpg', 'frame_174579.jpg', 'fram...     97\n",
              "\n",
              "[17947 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15cbf79",
      "metadata": {
        "id": "f15cbf79"
      },
      "source": [
        "### 2.2 Custom Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "RK6gHTVopf7M",
      "metadata": {
        "id": "RK6gHTVopf7M"
      },
      "outputs": [],
      "source": [
        "# cur_dir = \"data/qscale31_unique/\"\n",
        "cur_dir = \"/content/converted_224x224/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c21554c7",
      "metadata": {
        "id": "c21554c7"
      },
      "outputs": [],
      "source": [
        "# Define dataset class\n",
        "class MyDataSet(Dataset):\n",
        "\n",
        "    # load the dataset\n",
        "    def __init__(self, data, transform=None, **kwargs):\n",
        "        self.X = data[\"frames\"]\n",
        "        self.Y = data[\"label\"]\n",
        "        self.transform = transform\n",
        "\n",
        "    # get number of items/rows in dataset\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.X[index], self.Y[index]\n",
        "        for c in [\"(\",\")\",\",\",\"'\"]:\n",
        "            x = x.replace(c, \"\")\n",
        "        x = x.split(\" \")\n",
        "        images = []\n",
        "        for img_file in x:\n",
        "            img = Image.open(cur_dir + img_file)\n",
        "            img = torchvision.transforms.ToTensor()(img)\n",
        "            # img = torchvision.transforms.Normalize(mean=[0.2068, 0.2242, 0.2269], std=[0.2520, 0.2394, 0.2320])(img)\n",
        "            images.extend(img)\n",
        "        x = torch.stack(images)\n",
        "        if self.transform:\n",
        "          x = self.transform(x)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "mQAByEibn5iz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQAByEibn5iz",
        "outputId": "dac59bf9-1d21-470d-bfa2-1970b97b37d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([15, 224, 224])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set = MyDataSet(train_data)\n",
        "train_set[0][0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6b5efe",
      "metadata": {
        "id": "6d6b5efe"
      },
      "source": [
        "### 2.3 Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "T97n1WeOtugc",
      "metadata": {
        "id": "T97n1WeOtugc"
      },
      "outputs": [],
      "source": [
        "# transform_train = torchvision.transforms.Compose([\n",
        "#     torchvision.transforms.Normalize(mean=[0.4967, 0.4858, 0.4584, 0.4969, 0.4700, 0.4585, 0.4964, 0.4854, 0.4580, 0.4972, 0.4862, 0.4585, 0.4964, 0.4857, 0.4584],\n",
        "#                                       std=[0.2681, 0.2574, 0.2433, 0.2678, 0.2500, 0.2429, 0.2679, 0.2572, 0.2428, 0.2680, 0.2573, 0.2428, 0.2680, 0.2574, 0.2431])\n",
        "# ])\n",
        "\n",
        "transform_train = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomCrop(224, padding=28)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4fb2a207",
      "metadata": {
        "id": "4fb2a207"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# training data\n",
        "train_set = MyDataSet(train_data, transform_train)\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=8)\n",
        "\n",
        "# validation data\n",
        "val_set = MyDataSet(val_data)\n",
        "val_loader = DataLoader(val_set, shuffle=False, batch_size=batch_size, num_workers=8)\n",
        "\n",
        "# test data\n",
        "test_set = MyDataSet(test_data)\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a589effb",
      "metadata": {
        "id": "a589effb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "UUgfy9UUkavf",
      "metadata": {
        "id": "UUgfy9UUkavf"
      },
      "outputs": [],
      "source": [
        "# Code based on https://pytorch.org/vision/stable/_modules/torchvision/models/regnet.html\n",
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from typing import Any, Callable, List, Optional, Tuple\n",
        "from torch import nn, Tensor\n",
        "\n",
        "__all__ = [\"RegNet\", \"regnet_y_400mf\", \"regnet_y_800mf\", \"regnet_y_1_6gf\",\n",
        "           \"regnet_y_3_2gf\", \"regnet_y_8gf\", \"regnet_y_16gf\", \"regnet_y_32gf\",\n",
        "           \"regnet_x_400mf\", \"regnet_x_800mf\", \"regnet_x_1_6gf\", \"regnet_x_3_2gf\",\n",
        "           \"regnet_x_8gf\", \"regnet_x_16gf\", \"regnet_x_32gf\"]\n",
        "\n",
        "class ConvNormActivation(torch.nn.Sequential):\n",
        "    \"\"\"\n",
        "    Configurable block used for Convolution-Normalzation-Activation blocks.\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input image\n",
        "        out_channels (int): Number of channels produced by the Convolution-Normalzation-Activation block\n",
        "        kernel_size: (int, optional): Size of the convolving kernel. Default: 3\n",
        "        stride (int, optional): Stride of the convolution. Default: 1\n",
        "        padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in wich case it will calculated as ``padding = (kernel_size - 1) // 2 * dilation``\n",
        "        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
        "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolutiuon layer. If ``None`` this layer wont be used. Default: ``torch.nn.BatchNorm2d``\n",
        "        activation_layer (Callable[..., torch.nn.Module], optinal): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer wont be used. Default: ``torch.nn.ReLU``\n",
        "        dilation (int): Spacing between kernel elements. Default: 1\n",
        "        inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True``\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        padding: Optional[int] = None,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm2d,\n",
        "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
        "        dilation: int = 1,\n",
        "        inplace: bool = True,\n",
        "    ) -> None:\n",
        "        if padding is None:\n",
        "            padding = (kernel_size - 1) // 2 * dilation\n",
        "        layers = [\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                dilation=dilation,\n",
        "                groups=groups,\n",
        "                bias=norm_layer is None,\n",
        "            )\n",
        "        ]\n",
        "        if norm_layer is not None:\n",
        "            layers.append(norm_layer(out_channels))\n",
        "        if activation_layer is not None:\n",
        "            layers.append(activation_layer(inplace=inplace))\n",
        "        super().__init__(*layers)\n",
        "        # _log_api_usage_once(self)\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "\n",
        "class SqueezeExcitation(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    This block implements the Squeeze-and-Excitation block from https://arxiv.org/abs/1709.01507 (see Fig. 1).\n",
        "    Parameters ``activation``, and ``scale_activation`` correspond to ``delta`` and ``sigma`` in in eq. 3.\n",
        "    Args:\n",
        "        input_channels (int): Number of channels in the input image\n",
        "        squeeze_channels (int): Number of squeeze channels\n",
        "        activation (Callable[..., torch.nn.Module], optional): ``delta`` activation. Default: ``torch.nn.ReLU``\n",
        "        scale_activation (Callable[..., torch.nn.Module]): ``sigma`` activation. Default: ``torch.nn.Sigmoid``\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels: int,\n",
        "        squeeze_channels: int,\n",
        "        activation: Callable[..., torch.nn.Module] = torch.nn.ReLU,\n",
        "        scale_activation: Callable[..., torch.nn.Module] = torch.nn.Sigmoid,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        # _log_api_usage_once(self)\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = torch.nn.Conv2d(input_channels, squeeze_channels, 1)\n",
        "        self.fc2 = torch.nn.Conv2d(squeeze_channels, input_channels, 1)\n",
        "        self.activation = activation()\n",
        "        self.scale_activation = scale_activation()\n",
        "\n",
        "    def _scale(self, input: Tensor) -> Tensor:\n",
        "        scale = self.avgpool(input)\n",
        "        scale = self.fc1(scale)\n",
        "        scale = self.activation(scale)\n",
        "        scale = self.fc2(scale)\n",
        "        return self.scale_activation(scale)\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        scale = self._scale(input)\n",
        "        return scale * input\n",
        "\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class SimpleStemIN(ConvNormActivation):\n",
        "    \"\"\"Simple stem for ImageNet: 3x3, BN, ReLU.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        width_in: int,\n",
        "        width_out: int,\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        activation_layer: Callable[..., nn.Module],\n",
        "    ) -> None:\n",
        "        super().__init__(width_in, width_out, kernel_size=3, stride=2,\n",
        "                         norm_layer=norm_layer, activation_layer=activation_layer)\n",
        "\n",
        "\n",
        "class BottleneckTransform(nn.Sequential):\n",
        "    \"\"\"Bottleneck transformation: 1x1, 3x3 [+SE], 1x1.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        width_in: int,\n",
        "        width_out: int,\n",
        "        stride: int,\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        activation_layer: Callable[..., nn.Module],\n",
        "        group_width: int,\n",
        "        bottleneck_multiplier: float,\n",
        "        se_ratio: Optional[float],\n",
        "    ) -> None:\n",
        "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
        "        w_b = int(round(width_out * bottleneck_multiplier))\n",
        "        g = w_b // group_width\n",
        "\n",
        "        layers[\"a\"] = ConvNormActivation(width_in, w_b, kernel_size=1, stride=1,\n",
        "                                         norm_layer=norm_layer, activation_layer=activation_layer)\n",
        "        layers[\"b\"] = ConvNormActivation(w_b, w_b, kernel_size=3, stride=stride, groups=g,\n",
        "                                         norm_layer=norm_layer, activation_layer=activation_layer)\n",
        "\n",
        "        if se_ratio:\n",
        "            # The SE reduction ratio is defined with respect to the\n",
        "            # beginning of the block\n",
        "            width_se_out = int(round(se_ratio * width_in))\n",
        "            layers[\"se\"] = SqueezeExcitation(\n",
        "                input_channels=w_b,\n",
        "                squeeze_channels=width_se_out,\n",
        "                activation=activation_layer,\n",
        "            )\n",
        "\n",
        "        layers[\"c\"] = ConvNormActivation(w_b, width_out, kernel_size=1, stride=1,\n",
        "                                         norm_layer=norm_layer, activation_layer=None)\n",
        "        super().__init__(layers)\n",
        "\n",
        "\n",
        "class ResBottleneckBlock(nn.Module):\n",
        "    \"\"\"Residual bottleneck block: x + F(x), F = bottleneck transform.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        width_in: int,\n",
        "        width_out: int,\n",
        "        stride: int,\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        activation_layer: Callable[..., nn.Module],\n",
        "        group_width: int = 1,\n",
        "        bottleneck_multiplier: float = 1.0,\n",
        "        se_ratio: Optional[float] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Use skip connection with projection if shape changes\n",
        "        self.proj = None\n",
        "        should_proj = (width_in != width_out) or (stride != 1)\n",
        "        if should_proj:\n",
        "            self.proj = ConvNormActivation(width_in, width_out, kernel_size=1,\n",
        "                                           stride=stride, norm_layer=norm_layer, activation_layer=None)\n",
        "        self.f = BottleneckTransform(\n",
        "            width_in,\n",
        "            width_out,\n",
        "            stride,\n",
        "            norm_layer,\n",
        "            activation_layer,\n",
        "            group_width,\n",
        "            bottleneck_multiplier,\n",
        "            se_ratio,\n",
        "        )\n",
        "        self.activation = activation_layer(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.proj is not None:\n",
        "            x = self.proj(x) + self.f(x)\n",
        "        else:\n",
        "            x = x + self.f(x)\n",
        "        return self.activation(x)\n",
        "\n",
        "\n",
        "class AnyStage(nn.Sequential):\n",
        "    \"\"\"AnyNet stage (sequence of blocks w/ the same output shape).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        width_in: int,\n",
        "        width_out: int,\n",
        "        stride: int,\n",
        "        depth: int,\n",
        "        block_constructor: Callable[..., nn.Module],\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        activation_layer: Callable[..., nn.Module],\n",
        "        group_width: int,\n",
        "        bottleneck_multiplier: float,\n",
        "        se_ratio: Optional[float] = None,\n",
        "        stage_index: int = 0,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        for i in range(depth):\n",
        "            block = block_constructor(\n",
        "                width_in if i == 0 else width_out,\n",
        "                width_out,\n",
        "                stride if i == 0 else 1,\n",
        "                norm_layer,\n",
        "                activation_layer,\n",
        "                group_width,\n",
        "                bottleneck_multiplier,\n",
        "                se_ratio,\n",
        "            )\n",
        "\n",
        "            self.add_module(f\"block{stage_index}-{i}\", block)\n",
        "\n",
        "\n",
        "class BlockParams:\n",
        "    def __init__(\n",
        "        self,\n",
        "        depths: List[int],\n",
        "        widths: List[int],\n",
        "        group_widths: List[int],\n",
        "        bottleneck_multipliers: List[float],\n",
        "        strides: List[int],\n",
        "        se_ratio: Optional[float] = None,\n",
        "    ) -> None:\n",
        "        self.depths = depths\n",
        "        self.widths = widths\n",
        "        self.group_widths = group_widths\n",
        "        self.bottleneck_multipliers = bottleneck_multipliers\n",
        "        self.strides = strides\n",
        "        self.se_ratio = se_ratio\n",
        "\n",
        "    @classmethod\n",
        "    def from_init_params(\n",
        "        cls,\n",
        "        depth: int,\n",
        "        w_0: int,\n",
        "        w_a: float,\n",
        "        w_m: float,\n",
        "        group_width: int,\n",
        "        bottleneck_multiplier: float = 1.0,\n",
        "        se_ratio: Optional[float] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> \"BlockParams\":\n",
        "        \"\"\"\n",
        "        Programatically compute all the per-block settings,\n",
        "        given the RegNet parameters.\n",
        "\n",
        "        The first step is to compute the quantized linear block parameters,\n",
        "        in log space. Key parameters are:\n",
        "        - `w_a` is the width progression slope\n",
        "        - `w_0` is the initial width\n",
        "        - `w_m` is the width stepping in the log space\n",
        "\n",
        "        In other terms\n",
        "        `log(block_width) = log(w_0) + w_m * block_capacity`,\n",
        "        with `bock_capacity` ramping up following the w_0 and w_a params.\n",
        "        This block width is finally quantized to multiples of 8.\n",
        "\n",
        "        The second step is to compute the parameters per stage,\n",
        "        taking into account the skip connection and the final 1x1 convolutions.\n",
        "        We use the fact that the output width is constant within a stage.\n",
        "        \"\"\"\n",
        "\n",
        "        QUANT = 8\n",
        "        STRIDE = 2\n",
        "\n",
        "        if w_a < 0 or w_0 <= 0 or w_m <= 1 or w_0 % 8 != 0:\n",
        "            raise ValueError(\"Invalid RegNet settings\")\n",
        "        # Compute the block widths. Each stage has one unique block width\n",
        "        widths_cont = torch.arange(depth) * w_a + w_0\n",
        "        block_capacity = torch.round(torch.log(widths_cont / w_0) / math.log(w_m))\n",
        "        block_widths = (\n",
        "            torch.round(torch.divide(w_0 * torch.pow(w_m, block_capacity), QUANT))\n",
        "            * QUANT\n",
        "        ).int().tolist()\n",
        "        num_stages = len(set(block_widths))\n",
        "\n",
        "        # Convert to per stage parameters\n",
        "        split_helper = zip(\n",
        "            block_widths + [0],\n",
        "            [0] + block_widths,\n",
        "            block_widths + [0],\n",
        "            [0] + block_widths,\n",
        "        )\n",
        "        splits = [w != wp or r != rp for w, wp, r, rp in split_helper]\n",
        "\n",
        "        stage_widths = [w for w, t in zip(block_widths, splits[:-1]) if t]\n",
        "        stage_depths = torch.diff(torch.tensor([d for d, t in enumerate(splits) if t])).int().tolist()\n",
        "\n",
        "        strides = [STRIDE] * num_stages\n",
        "        bottleneck_multipliers = [bottleneck_multiplier] * num_stages\n",
        "        group_widths = [group_width] * num_stages\n",
        "\n",
        "        # Adjust the compatibility of stage widths and group widths\n",
        "        stage_widths, group_widths = cls._adjust_widths_groups_compatibilty(\n",
        "            stage_widths, bottleneck_multipliers, group_widths\n",
        "        )\n",
        "\n",
        "        return cls(\n",
        "            depths=stage_depths,\n",
        "            widths=stage_widths,\n",
        "            group_widths=group_widths,\n",
        "            bottleneck_multipliers=bottleneck_multipliers,\n",
        "            strides=strides,\n",
        "            se_ratio=se_ratio,\n",
        "        )\n",
        "\n",
        "    def _get_expanded_params(self):\n",
        "        return zip(\n",
        "            self.widths, self.strides, self.depths, self.group_widths, self.bottleneck_multipliers\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _adjust_widths_groups_compatibilty(\n",
        "            stage_widths: List[int], bottleneck_ratios: List[float],\n",
        "            group_widths: List[int]) -> Tuple[List[int], List[int]]:\n",
        "        \"\"\"\n",
        "        Adjusts the compatibility of widths and groups,\n",
        "        depending on the bottleneck ratio.\n",
        "        \"\"\"\n",
        "        # Compute all widths for the current settings\n",
        "        widths = [int(w * b) for w, b in zip(stage_widths, bottleneck_ratios)]\n",
        "        group_widths_min = [min(g, w_bot) for g, w_bot in zip(group_widths, widths)]\n",
        "\n",
        "        # Compute the adjusted widths so that stage and group widths fit\n",
        "        ws_bot = [_make_divisible(w_bot, g) for w_bot, g in zip(widths, group_widths_min)]\n",
        "        stage_widths = [int(w_bot / b) for w_bot, b in zip(ws_bot, bottleneck_ratios)]\n",
        "        return stage_widths, group_widths_min\n",
        "\n",
        "\n",
        "class RegNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block_params: BlockParams,\n",
        "        in_channels: int = 3,\n",
        "        num_classes: int = 1000,\n",
        "        stem_width: int = 32,\n",
        "        stem_type: Optional[Callable[..., nn.Module]] = None,\n",
        "        block_type: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        activation: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if stem_type is None:\n",
        "            stem_type = SimpleStemIN\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if block_type is None:\n",
        "            block_type = ResBottleneckBlock\n",
        "        if activation is None:\n",
        "            activation = nn.ReLU\n",
        "\n",
        "        # Ad hoc stem\n",
        "        self.stem = stem_type(\n",
        "            in_channels,  # width_in\n",
        "            stem_width,\n",
        "            norm_layer,\n",
        "            activation,\n",
        "        )\n",
        "\n",
        "        current_width = stem_width\n",
        "\n",
        "        blocks = []\n",
        "        for i, (\n",
        "            width_out,\n",
        "            stride,\n",
        "            depth,\n",
        "            group_width,\n",
        "            bottleneck_multiplier,\n",
        "        ) in enumerate(block_params._get_expanded_params()):\n",
        "            blocks.append(\n",
        "                (\n",
        "                    f\"block{i+1}\",\n",
        "                    AnyStage(\n",
        "                        current_width,\n",
        "                        width_out,\n",
        "                        stride,\n",
        "                        depth,\n",
        "                        block_type,\n",
        "                        norm_layer,\n",
        "                        activation,\n",
        "                        group_width,\n",
        "                        bottleneck_multiplier,\n",
        "                        block_params.se_ratio,\n",
        "                        stage_index=i + 1,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            current_width = width_out\n",
        "\n",
        "        self.trunk_output = nn.Sequential(OrderedDict(blocks))\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(in_features=current_width, out_features=num_classes)\n",
        "\n",
        "        # Init weights and good to go\n",
        "        self._reset_parameters()\n",
        "\n",
        "        # initial weights\n",
        "        for m in self.modules():\n",
        "          if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "          elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "          elif isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, 0, 0.01)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.stem(x)\n",
        "        x = self.trunk_output(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _reset_parameters(self) -> None:\n",
        "        # Performs ResNet-style weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # Note that there is no bias due to BN\n",
        "                fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=math.sqrt(2.0 / fan_out))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "def _regnet(arch: str, block_params: BlockParams, in_channels, num_classes, pretrained: bool, progress: bool, **kwargs: Any) -> RegNet:\n",
        "    model = RegNet(block_params, in_channels=in_channels, num_classes=num_classes, norm_layer=partial(nn.BatchNorm2d, eps=1e-05, momentum=0.1), **kwargs)\n",
        "    if pretrained:\n",
        "        if arch not in model_urls:\n",
        "            raise ValueError(f\"No checkpoint is available for model type {arch}\")\n",
        "        # state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
        "        # model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "def regnet_y_400mf(in_channels, num_classes, pretrained: bool = False, progress: bool = True, **kwargs: Any) -> RegNet:\n",
        "    \"\"\"\n",
        "    Constructs a RegNetY_400MF architecture from\n",
        "    `\"Designing Network Design Spaces\" <https://arxiv.org/abs/2003.13678>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    params = BlockParams.from_init_params(depth=16, w_0=48, w_a=27.89, w_m=2.09,\n",
        "                                          group_width=8, se_ratio=0.25, **kwargs)\n",
        "    return _regnet(\"regnet_y_400mf\", params, in_channels, num_classes, pretrained, progress, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3kbzpQsuQbOA",
      "metadata": {
        "id": "3kbzpQsuQbOA"
      },
      "outputs": [],
      "source": [
        "class ClassificationNetwork(torch.nn.Module):\n",
        "    def __init__(self, in_channels, embedding, num_classes):\n",
        "        super().__init__()\n",
        "        self.cnn = regnet_y_400mf(in_channels, embedding)\n",
        "        self.mlp = nn.Sequential(nn.Linear(embedding,num_classes))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.cnn(x)\n",
        "      out = self.mlp(out)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "964a4c9e",
      "metadata": {
        "id": "964a4c9e"
      },
      "outputs": [],
      "source": [
        "numEpochs = 100\n",
        "in_features = 15 # TODO: change RGB channels according to num of frames\n",
        "embedding = 512\n",
        "\n",
        "learningRate = 0.1\n",
        "weightDecay = 1e-4\n",
        "\n",
        "num_classes = 120\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "network = ClassificationNetwork(in_features, embedding, num_classes)\n",
        "#network.load_state_dict(torch.load(\"model_checkpoints/resnet34/lr_0.1-2/model_2.pt\"))\n",
        "network = network.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2TdiwHBaNFvt",
      "metadata": {
        "id": "2TdiwHBaNFvt"
      },
      "outputs": [],
      "source": [
        "# Train!\n",
        "max_val_acc = 0\n",
        "for epoch in range(numEpochs):\n",
        "    # Train\n",
        "    network.train()\n",
        "    avg_loss = 0.0\n",
        "    avg_train_acc = 0.0\n",
        "    for batch_num, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x.requires_grad_()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = network(x)\n",
        "        num_train_correct = (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "        num_labels = len(y)\n",
        "        avg_train_acc += (num_train_correct/num_labels)\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "        if batch_num % 50 == 49:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}\\tTraining Accuracy : {:.4f}'.format(epoch, batch_num+1, avg_loss/50, avg_train_acc/50))\n",
        "            avg_loss = 0.0\n",
        "            avg_train_acc = 0.0\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        del x\n",
        "        del y\n",
        "        del loss\n",
        "    \n",
        "    # Validate\n",
        "    with torch.no_grad():\n",
        "      network.eval()\n",
        "      avg_val_loss = 0.0\n",
        "      num_correct = 0\n",
        "      for batch_num, (x, y) in enumerate(val_loader):\n",
        "          x, y = x.to(device), y.to(device)\n",
        "          outputs = network(x)\n",
        "          num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "          loss = criterion(outputs, y.long())\n",
        "          avg_val_loss += loss.item()\n",
        "\n",
        "          torch.cuda.empty_cache()\n",
        "          del x\n",
        "          del y\n",
        "\n",
        "      avg_val_loss = avg_val_loss / len(val_loader)\n",
        "      val_acc = num_correct / len(val_set)\n",
        "      checkpoint_name = \"/content/gdrive/MyDrive/regnet_ours_data5/model_\" + str(epoch) + \".pt\"\n",
        "      torch.save(network.state_dict(), checkpoint_name)\n",
        "      if val_acc > max_val_acc:\n",
        "          max_val_acc = val_acc\n",
        "          torch.save(network.state_dict(), \"/content/gdrive/MyDrive/regnet_ours_data5/best_model.pt\")\n",
        "    scheduler.step(avg_val_loss)\n",
        "    print('Epoch: {}, Validation Loss: {:.3f}, Validation Accuracy: {:.3f}'.format(epoch, avg_val_loss, val_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SHZTDySnc_80",
      "metadata": {
        "id": "SHZTDySnc_80"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "t7XRDDeQddip",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "0a1fe6e90d46446688cb39c2d0fcbc68",
            "ed7724d7cffb4f9b8dc87fca9cdf7951",
            "77adee7d2ad84a28a3728d22ac55549a",
            "d6f055c743264db18b6a140cfcc3217d",
            "619d336975f04b01804ecd0cc3d369f2",
            "a1756b07b35b463592cfaccf4671dcb6",
            "755bb6507ff04c869c0711c5994f179c",
            "8fa2ca27c76a41f691b69233b7662995",
            "4d12f1a82dc243808ea980a6d6ea020e",
            "bdff40d4f0054dadbaac74f95b5eb48d",
            "907f2c747d004d118ebde2bf112a0587"
          ]
        },
        "id": "t7XRDDeQddip",
        "outputId": "50282352-9f9c-410a-96b4-548de7d3602f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a1fe6e90d46446688cb39c2d0fcbc68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2836685797069148\n"
          ]
        }
      ],
      "source": [
        "network.eval()\n",
        "num_correct = 0\n",
        "for x, y in tqdm(test_loader):\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  outputs = network(x)\n",
        "  num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "test_acc = num_correct / len(test_set)\n",
        "print(test_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "RegNet_15channels_direct_DATA5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a1fe6e90d46446688cb39c2d0fcbc68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77adee7d2ad84a28a3728d22ac55549a",
              "IPY_MODEL_d6f055c743264db18b6a140cfcc3217d",
              "IPY_MODEL_619d336975f04b01804ecd0cc3d369f2"
            ],
            "layout": "IPY_MODEL_ed7724d7cffb4f9b8dc87fca9cdf7951"
          }
        },
        "4d12f1a82dc243808ea980a6d6ea020e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "619d336975f04b01804ecd0cc3d369f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_907f2c747d004d118ebde2bf112a0587",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bdff40d4f0054dadbaac74f95b5eb48d",
            "value": " 141/141 [01:29&lt;00:00,  3.99it/s]"
          }
        },
        "755bb6507ff04c869c0711c5994f179c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77adee7d2ad84a28a3728d22ac55549a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_755bb6507ff04c869c0711c5994f179c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a1756b07b35b463592cfaccf4671dcb6",
            "value": "100%"
          }
        },
        "8fa2ca27c76a41f691b69233b7662995": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "907f2c747d004d118ebde2bf112a0587": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1756b07b35b463592cfaccf4671dcb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdff40d4f0054dadbaac74f95b5eb48d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6f055c743264db18b6a140cfcc3217d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d12f1a82dc243808ea980a6d6ea020e",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fa2ca27c76a41f691b69233b7662995",
            "value": 141
          }
        },
        "ed7724d7cffb4f9b8dc87fca9cdf7951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
